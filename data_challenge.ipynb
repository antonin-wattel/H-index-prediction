{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INF554 Machine and Deep Learning\n",
    "# Data Challenge: H-index Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexandre Hirsch, Antonin Wattel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import gc\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.9s\n",
    "\n",
    "# read training data\n",
    "df_train = pd.read_csv('train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this may be useful for preprocessing\n",
    "#https://kavita-ganesan.com/text-preprocessing-tutorial/#.XHa4-ZNKhuU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded\n"
     ]
    }
   ],
   "source": [
    "#different pre-trained word-embedding models\n",
    "\n",
    "wv = api.load('word2vec-google-news-300') #best\n",
    "#wv = api.load('glove-wiki-gigaword-300')\n",
    "#wv = api.load('conceptnet-numberbatch-17-06-300')\n",
    "#wv = Word2Vec.load(\"w2v.model\").wv\n",
    "\n",
    "print(\"Word2Vec model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version with translator (not used)\n",
    "\n",
    "# translator = Translator()\n",
    "# def store_abstracts_translated():\n",
    "#     paper_IDs = dict()\n",
    "#     with open('data/abstracts.txt') as f:\n",
    "#         for l in f:\n",
    "#             paper_ID, abstract = l.split(\"----\",1)\n",
    "#             d = json.loads(abstract)\n",
    "#             InvertedIndex, IndexLength = d[\"InvertedIndex\"], d[\"IndexLength\"]\n",
    "#             words = InvertedIndex.keys()\n",
    "\n",
    "#             detected = False\n",
    "#             while not detected:\n",
    "#                 try:\n",
    "#                     language_code = translator.detect(' '.join(list(words)[:10])).lang\n",
    "#                     detected = True\n",
    "#                 except:\n",
    "#                     print(f\"can't detect for paper {paper_ID}, waiting two seconds\")\n",
    "#                     time.sleep(2)\n",
    "#                     continue\n",
    "\n",
    "#             if language_code != 'en':\n",
    "#                 ab = [None]*IndexLength\n",
    "#                 for word, v in InvertedIndex.items():\n",
    "#                     for i in v:\n",
    "#                         ab[i] = word\n",
    "#                 translated = False\n",
    "#                 while not translated:\n",
    "#                     try:\n",
    "#                         words = set(translator.translate(' '.join(list(filter(None, ab))), src = language_code, dest = 'en').text.split())\n",
    "#                         translated = True\n",
    "#                     except:\n",
    "#                         print(f\"can't translate for paper {paper_ID}, waiting two seconds\")\n",
    "#                         time.sleep(2)\n",
    "#                         continue\n",
    "#             paper_IDs[int(paper_ID)] = list(words - STOPWORDS)\n",
    "#     # with open('data/stopped_translated_abstracts.json', 'w') as f:\n",
    "#     #     json.dump(paper_IDs, f)\n",
    "#     return paper_IDs\n",
    "\n",
    "# paper_IDs = store_abstracts_translated()\n",
    "# print('abstracts stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstracts stored\n"
     ]
    }
   ],
   "source": [
    "#without translation\n",
    "def store_abstracts():\n",
    "    paper_IDs = dict()\n",
    "    #with open('abstracts.txt') as f:\n",
    "    with open('abstracts.txt', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            paper_ID, abstract = l.split(\"----\",1)\n",
    "            #sent = [None]*d[\"IndexLength\"]\n",
    "            #for word, v in d[\"InvertedIndex\"].items():\n",
    "                #for i in v:\n",
    "                    #sent[i] = word\n",
    "            #sentences += nltk.sent_tokenize(' '.join(list(filter(None, sent))))\n",
    "            paper_IDs[int(paper_ID)] = json.loads(abstract)[\"InvertedIndex\"].keys() - STOPWORDS\n",
    "    return paper_IDs #, sentences\n",
    "\n",
    "paper_IDs = store_abstracts()\n",
    "print('abstracts stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors stored\n"
     ]
    }
   ],
   "source": [
    "#12 s\n",
    "def store_authors():\n",
    "    author_IDs = dict()\n",
    "    with open('author_papers.txt') as f:\n",
    "        for l in f:\n",
    "            author_ID, papers = l.split(':')\n",
    "            author_IDs[int(author_ID)] = list(map(int,papers.split('-')))\n",
    "    return author_IDs\n",
    "    \n",
    "author_IDs = store_authors()\n",
    "print('authors stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = wv.vector_size\n",
    "\n",
    "def get_paper_value(paper_ID):\n",
    "    vec = np.zeros(wv.vector_size)\n",
    "    try:\n",
    "        words_used = set()\n",
    "        for token in paper_IDs[paper_ID]:\n",
    "            words = re.sub(r'[-/]', ' ', re.sub(r'[.…,:?!;\\'‘’\"“”()*–]|[0-9]+-|[0-9]|\\'s', '', token))\n",
    "            for w in words.split():\n",
    "                if w not in STOPWORDS and w not in words_used:\n",
    "                    words_used.add(w)\n",
    "                    try:\n",
    "                        vec += wv[w]\n",
    "                    except:\n",
    "                        continue\n",
    "    except:\n",
    "        pass\n",
    "    return vec\n",
    "    \n",
    "def get_author_value(author_ID):\n",
    "    vec = np.zeros(wv.vector_size)\n",
    "    for paper_ID in author_IDs[author_ID]:\n",
    "        vec += get_paper_value(paper_ID)\n",
    "    return vec/len(author_IDs[author_ID])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data loaded\n",
      "testing data loaded\n"
     ]
    }
   ],
   "source": [
    "#6m\n",
    "\n",
    "X_train = np.zeros((n_train, wv.vector_size))\n",
    "#y_train = np.zeros(n_train)\n",
    "\n",
    "for i,row in df_train.iterrows():\n",
    "    author = row['author']\n",
    "    X_train[i,:] = get_author_value(author)\n",
    "    #y_train[i] = row['hindex']\n",
    "\n",
    "print('training data loaded')\n",
    "\n",
    "X_test = np.zeros((n_test, wv.vector_size))\n",
    "for i,row in df_test.iterrows():\n",
    "    author = row['author']\n",
    "    X_test[i,:] = get_author_value(author)\n",
    "print('testing data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save feature vectors from the abstracts\n",
    "\n",
    "np.save('X_train_abstract_google.npy', X_train)\n",
    "np.save('X_test_abstract_google.npy', X_test)\n",
    "# np.save('X_train_abstract_gigaword.npy', X_train)\n",
    "# np.save('X_test_abstract_gigaword.npy', X_test)\n",
    "# np.save('X_train_abstract_numberbatch.npy', X_train)\n",
    "# np.save('X_test_abstract_numberbatch.npy', X_test)\n",
    "# np.save('X_train_abstract_custom.npy', X_train)\n",
    "# np.save('X_test_abstract_custom.npy', X_test)\n",
    "\n",
    "del X_train\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217801\n",
      "Number of edges: 1718164\n"
     ]
    }
   ],
   "source": [
    "#5s\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#convert IDs to numeric indices in order to use karate library\n",
    "\n",
    "mapping_to_indices = { node : i for node, i in zip([node for node in G.nodes()], [i for i in range(n_nodes)]) }\n",
    "inverse_mapping = dict(zip(mapping_to_indices.values(),mapping_to_indices.keys()))\n",
    "G = nx.relabel_nodes(G, mapping_to_indices)\n",
    "node_indices = [node for node in G.nodes()]\n",
    "print(\"node = \", node_indices[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrality based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast\n",
    "core_number = nx.core_number(G)\n",
    "print('core_number')\n",
    "page_rank = nx.pagerank(G) #not so sure about the alpha\n",
    "print('pagerank')\n",
    "#nb_triangles = nx.triangles(G)\n",
    "#print('triangles')\n",
    "#deg_centrality = nx.degree_centrality(G) \n",
    "#print('deg_centrality')\n",
    "# eig_centrality = nx.eigenvector_centrality(G)\n",
    "# print('eigenvector_centrality')\n",
    "\n",
    "#slow: >20 mins\n",
    "# close_centrality = nx.closeness_centralsity(G)\n",
    "# print('closeness_centrality')\n",
    "#bet_centrality = nx.betweenness_centrality(G, normalized = True, endpoints = False)\n",
    "# print('bet_centrality')\n",
    "# katz_centrality = nx.katz_centrality(G)\n",
    "# print('katz_centrality')\n",
    "# current_flow_closeness_centrality = nx.current_flow_closeness_centrality(G)\n",
    "# print('current_flow')\n",
    "# current_flow_betweenness_centrality = nx.current_flow_betweenness_centrality(G)\n",
    "# print('current_flow_betweenness_centrality')\n",
    "# load_centrality = nx.load_centrality(G)\n",
    "# print('load_centrality')\n",
    "# harmonic_centrality = nx.harmonic_centrality(G)\n",
    "# print('harmonic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((n_train, 3))\n",
    "\n",
    "for i,row in df_train.iterrows():\n",
    "    author = row['author']\n",
    "    X_train[i, 0] = G.degree(author)\n",
    "    X_train[i, 1] = core_number[author]\n",
    "    X_train[i, 2] = page_rank[author] #actually this is only useful for directed networks\n",
    "    #X_train[i, 3] = nb_triangles[author]\n",
    "    #X_train[i, 4] = deg_centrality[author]\n",
    "    # X_train[i, 3] = eig_centrality[author]\n",
    "\n",
    "\n",
    "    # X_train[i, 6] = close_centrality[author]\n",
    "    #X_train[i, 5] = bet_centrality[author]\n",
    "    # X_train[i, 8] = katz_centrality[author]\n",
    "    # X_train[i, 9] = current_flow_closeness_centrality[author]\n",
    "    # X_train[i, 10] = current_flow_betweenness_centrality[author]\n",
    "    # X_train[i, 11] = load_centrality[author]\n",
    "    # X_train[i, 12] = harmonic_centrality[author]\n",
    "    # X_train[i, 13] = percolation_centrality[author]\n",
    "    # X_train[i, 14] = second_order_centrality[author]\n",
    "    # X_train[i, 15] = trophic_levels[author]\n",
    "\n",
    "print('loaded training features')\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "X_test = np.zeros((n_test, 3))\n",
    "\n",
    "for i,row in df_test.iterrows():\n",
    "    author = row['author']\n",
    "    X_test[i, 0] = G.degree(author)\n",
    "    X_test[i, 1] = core_number[author]\n",
    "    X_test[i, 2] = page_rank[author]\n",
    "    #X_test[i, 3] = nb_triangles[author]\n",
    "    #X_test[i, 4] = deg_centrality[author]\n",
    "    # X_test[i, 3] = eig_centrality[author]\n",
    "\n",
    "    # X_test[i, 6] = close_centrality[author]\n",
    "    #X_test[i, 5] = bet_centrality[author]\n",
    "    # X_test[i, 8] = katz_centrality[author]\n",
    "    # X_test[i, 9] = current_flow_closeness_centrality[author]\n",
    "    # X_test[i, 10] = current_flow_betweenness_centrality[author]\n",
    "    # X_test[i, 11] = load_centrality[author]\n",
    "    # X_test[i, 12] = harmonic_centrality[author]\n",
    "    # X_test[i, 13] = percolation_centrality[author]\n",
    "    # X_test[i, 14] = second_order_centrality[author]\n",
    "    # X_test[i, 15] = trophic_levels[author]\n",
    "\n",
    "print('loaded testing features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_graph.npy', X_train)\n",
    "np.save('X_test_graph.npy', X_test)\n",
    "\n",
    "del X_train\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximity preserving node embeddings\n",
    "https://github.com/benedekrozemberczki/karateclub/tree/master/karateclub/node_embedding/neighbourhood#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a few hours to run...\n",
    "#380 m\n",
    "\n",
    "from karateclub.node_embedding.neighbourhood import Node2Vec \n",
    "model = Node2Vec()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('Node2Vec_embedding.npy', embedding )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#about 20 m\n",
    "from karateclub.node_embedding.neighbourhood import BoostNE\n",
    "\n",
    "model = BoostNE()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('BoostNE_embedding.npy', embedding )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30s\n",
    "from karateclub.node_embedding.neighbourhood import NetMF\n",
    "\n",
    "model = NetMF()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('NetMF_embedding.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16m\n",
    "from karateclub.node_embedding.neighbourhood import DeepWalk\n",
    "\n",
    "model = DeepWalk()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('DeepWalk_embedding.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 s\n",
    "\n",
    "from karateclub.node_embedding.neighbourhood import RandNE\n",
    "\n",
    "model = RandNE()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('RandNE_embedding.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>17 m \n",
    "from karateclub.node_embedding.neighbourhood import GraRep\n",
    "\n",
    "model = GraRep()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('GraRep.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a few hours\n",
    "#136min\n",
    "from karateclub.node_embedding.neighbourhood import Diff2Vec\n",
    "\n",
    "model = Diff2Vec()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('Diff2Vec.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23 m\n",
    "from karateclub.node_embedding.neighbourhood import Walklets\n",
    "\n",
    "model = Walklets()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('Walklets.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#81 m\n",
    "model = Walklets(walk_number = 15, walk_length = 100, dimensions = 64, workers = 16)\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('Walklets_2.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#241 m\n",
    "model = Walklets(walk_number = 20, walk_length = 120, dimensions = 100, workers = 16)\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('Walklets_3.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import NMFADMM\n",
    "\n",
    "model =  NMFADMM()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('NMFADMM.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import LaplacianEigenmaps\n",
    "\n",
    "model = LaplacianEigenmaps()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('LaplacianEigenmaps.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import HOPE\n",
    "\n",
    "model = HOPE()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('HOPE.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import NodeSketch\n",
    "\n",
    "model = NodeSketch()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('NodeSketch.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import GLEE\n",
    "\n",
    "model = GLEE()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('GLEE.npy', embedding  )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Node Level embeddings\n",
    "\n",
    "https://github.com/benedekrozemberczki/karateclub/tree/master/karateclub/node_embedding/structural\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "from karateclub.node_embedding.structural import Role2Vec\n",
    "\n",
    "model = Role2Vec()\n",
    "model.fit(G)\n",
    "embedding = model.get_embedding()\n",
    "np.save('Role2Vec_embedding.npy', embedding )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from karateclub.node_embedding.meta import NEU\n",
    "from karateclub.node_embedding.neighbourhood import Walklets\n",
    "\n",
    "model = Walklets()\n",
    "meta_model = NEU()\n",
    "meta_model.fit(G, model)\n",
    "embedding = meta_model.get_embedding()\n",
    "np.save('NEU_Walklets.npy', embedding )\n",
    "\n",
    "del embedding\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking node embeddings + dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oyea\n",
      "(174241, 1256)\n"
     ]
    }
   ],
   "source": [
    "def stack_features(features_list):\n",
    "    X_train = np.array([])\n",
    "    X_test = np.array([])\n",
    "    for name in features_list:\n",
    "        X_train_path = 'X_train_'+name+ '.npy'\n",
    "        X_test_path = 'X_test_'+name+ '.npy'\n",
    "        a = normalize(np.load(X_train_path).astype(float))\n",
    "        b = normalize(np.load(X_test_path).astype(float))\n",
    "        if len(X_train) ==0:\n",
    "            X_train = a\n",
    "            X_test = b\n",
    "        else:\n",
    "            #print(a.shape)\n",
    "            X_train = np.concatenate((X_train,a), axis=1)\n",
    "            X_test = np.concatenate((X_test, b), axis=1)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ['Node2Vec', 'BoostNE', 'NetMF', 'RandNE', \n",
    "        'Deepwalk','Diff2Vec', 'Role2Vec','Walklets', \n",
    "        'NMFADMM', 'LaplacianEigenmaps', 'HOPE']\n",
    "\n",
    "X_train, X_test = stack_features(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now perform dimensionality reduction using pca\n",
    "#30s\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "principal=PCA(n_components=150)\n",
    "principal.fit(X_train)\n",
    "x=principal.transform(X_train)\n",
    "\n",
    "a=principal.transform(X_train)\n",
    "b=principal.transform(X_test)\n",
    "\n",
    "np.save('X_train_pca.npy', a)\n",
    "np.save('X_test_pca.npy', b)\n",
    "\n",
    "\n",
    "#use another form of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading features from saved graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 s\n",
    "\n",
    "#make sure this is right\n",
    "def load_and_save_from_embedding(embedding_string):\n",
    "\n",
    "    #path = embedding_string+ '_embedding.npy'\n",
    "    path = embedding_string+ '.npy'\n",
    "    if not os.path.exists(path):\n",
    "        print(path+ ' does not exist')\n",
    "        return 0\n",
    "        \n",
    "    embedding = np.load(path) \n",
    "    #to do: try_ except\n",
    "\n",
    "    X_train = np.zeros((n_train, len(embedding[0])))\n",
    "    y_train = np.zeros(n_train)\n",
    "    #print(X_train.shape)\n",
    "\n",
    "    for i,row in df_train.iterrows():\n",
    "        author = row['author']\n",
    "        #j = int(np.where(original_indices == author)[0]) #I guess this is really not efficient\n",
    "        j = mapping_to_indices[author]#not so sure\n",
    "        X_train[i,:] = embedding[j]\n",
    "        #y_train[i] = row['hindex']\n",
    "\n",
    "    #print('training data loaded')\n",
    "\n",
    "    X_test = np.zeros((n_test, len(embedding[0])))\n",
    "    for i,row in df_test.iterrows():\n",
    "        author = row['author']\n",
    "        #j = np.where(np.array(original_indices) == author)[0]\n",
    "        j = mapping_to_indices[author]\n",
    "        X_test[i,:] = embedding[j]\n",
    "\n",
    "    #print('testing data loaded')\n",
    "\n",
    "    np.save('X_train_'+embedding_string+'.npy', X_train)\n",
    "    np.save('X_test_'+embedding_string+'.npy', X_test)\n",
    "\n",
    "    del X_train\n",
    "    del X_test\n",
    "    gc.collect()    \n",
    "\n",
    "    print(embedding_string+' done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = ['Node2Vec', 'GraRep', 'BoostNE', 'NetMF', 'RandNE', \n",
    "                 'Deepwalk','Diff2Vec', 'Role2Vec', 'Walklets', 'NMFADMM',\n",
    "                 'LaplacianEigenmaps', 'HOPE', 'SocioDim', 'NodeSketch',\n",
    "                 'Walklets_2', 'Walklets_3','NEU_Walklets']\n",
    "\n",
    "for embedding in embedding_list:\n",
    "    load_and_save_from_embedding(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging features\n",
    "\n",
    "Now that we have features, we can assemble them and perform regression to get our prediction model.<br/>\n",
    "here, we load pre-computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    y_train[i] = row['hindex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.load('X_train_abstract.npy').astype(float)\n",
    "a2 = np.load('X_train_graph.npy').astype(float)\n",
    "a3 = np.load('X_train_Walklets_2.npy').astype(float)\n",
    "\n",
    "b1 = np.load('X_test_abstract.npy').astype(float)\n",
    "b2 = np.load('X_test_graph.npy').astype(float)\n",
    "b3 = np.load('X_test_Walklets_2.npy').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174241, 559)\n",
      "(43560, 559)\n"
     ]
    }
   ],
   "source": [
    "#stack features \n",
    "\n",
    "X_train = np.concatenate((a1, a2, a3), axis=1)\n",
    "X_test = np.concatenate((b1, b2, b3), axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training set into training_1 and validation\n",
    "\n",
    "def shuffle_split(X_train, y_train):\n",
    "    #shuffle\n",
    "    s = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    X_train  = X_train[s]\n",
    "    y_train = y_train[s]\n",
    "\n",
    "    #split\n",
    "    X_train_1 = X_train[len(df_train)//5:]\n",
    "    y_train_1 = y_train[len(df_train)//5:]\n",
    "    X_validation= X_train[:len(df_train)//5]\n",
    "    y_validation = y_train[:len(df_train)//5]\n",
    "\n",
    "    return X_train_1, y_train_1, X_validation, y_validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, y_train_1, X_validation, y_validation = shuffle_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_pred_1, y_validation):\n",
    "    mse = (np.square(y_pred_1 - y_validation)).mean(axis=0)\n",
    "    #print(y_pred_1[10:20])\n",
    "    #print(y_validation[10:20])\n",
    "    print('MSE =',  mse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Different node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_graph_pipeline(embedding_string, y_train, X_train_1=0, y_train_1=0, X_validation=0, y_validation=0):\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(embedding_string)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "\n",
    "    if embedding_string != 'No':\n",
    "        X_train_path = 'X_train_'+embedding_string+ '.npy'\n",
    "        X_test_path = 'X_test_'+embedding_string+ '.npy'\n",
    "\n",
    "        if not os.path.exists(X_train_path) or not os.path.exists(X_test_path):\n",
    "            print('path does not exist')\n",
    "            return 1000000000000\n",
    "        \n",
    "        X_train = np.load(X_train_path).astype(float)\n",
    "        X_test = np.load(X_test_path).astype(float) \n",
    "\n",
    "        #split training set into training_1 and validation\n",
    "\n",
    "        \n",
    "        #X_train_1, y_train_1, X_validation, y_validation = shuffle_split(X_train, X_test)\n",
    "        #shuffle\n",
    "        s = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        X_train  = X_train[s]\n",
    "        y_train = y_train[s]\n",
    "\n",
    "        #split\n",
    "        X_train_1 = X_train[len(df_train)//5:]\n",
    "        y_train_1 = y_train[len(df_train)//5:]\n",
    "        X_validation= X_train[:len(df_train)//5]\n",
    "        y_validation = y_train[:len(df_train)//5]\n",
    "\n",
    "    #now time to do a regression\n",
    "\n",
    "    #to do: use this as a parameter\n",
    "    reg1 = MLPRegressor(hidden_layer_sizes=(150), activation='relu', solver='adam', alpha=0.001, verbose = 0, max_iter=100, tol = 1*pow(10, -6), n_iter_no_change=10, early_stopping=True)#use pytorch implementation instead (way faster)\n",
    "    reg2 = Lasso()\n",
    "    reg3 = sklearn.linear_model.ARDRegression()#MSE=86 in 10 iterations\n",
    "    reg4 = sklearn.linear_model.BayesianRidge() #MSE=86 in 8 iterations\n",
    "    reg5  = sklearn.linear_model.SGDRegressor(loss='squared_error', early_stopping=True, n_iter_no_change=10)\n",
    "    #reg6 = svm.SVR()\n",
    "    reg7 = KNeighborsRegressor(n_neighbors = 20)\n",
    "    #reg8 = tree.DecisionTreeRegressor()\n",
    "\n",
    "    \n",
    "    regressors = [reg1, reg2, reg3, reg4, reg5, reg7]\n",
    "    mse = np.zeros(len(regressors))\n",
    "    \n",
    "    i=0\n",
    "    for reg in regressors:\n",
    "        print(str(reg))\n",
    "        reg.fit(X_train_1, y_train_1)\n",
    "        #plt.plot(reg.loss_curve_, label = embedding_string)\n",
    "        #plt.legend(embedding_string)\n",
    "        #plt.title(embedding_string)\n",
    "        y_pred_1 = reg.predict(X_validation)\n",
    "        mse[i]  = compute_mse(y_pred_1, y_validation)\n",
    "        print('mse =', mse[i])\n",
    "        i+=1\n",
    "        \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes quite some time\n",
    "\n",
    "embedding_list = ['Node2Vec', 'BoostNE', 'NetMF', 'RandNE', \n",
    "                'Deepwalk','Diff2Vec', 'Role2Vec','Walklets',\n",
    "                 'Walklets_2', 'Walklets_3', 'NEU_Walklets', \n",
    "                 'NMFADMM', 'LaplacianEigenmaps', 'HOPE', 'SocioDim', 'pca']\n",
    "\n",
    "i=0\n",
    "mses  = np.zeros((len(embedding_list), 6))\n",
    "\n",
    "for embedding_string in embedding_list :\n",
    "    mse = train_test_graph_pipeline(embedding_string, y_train)\n",
    "    mses[i] = mse\n",
    "    i+=1\n",
    "\n",
    "print(mses)\n",
    "mses = np.around(mses, decimals=2)\n",
    "np.savetxt('embedding_results.csv', mses, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Different word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wes = ['abstract_gigaword', 'abstract_google', 'abstract_numberbatch',  'abstract_custom']\n",
    "mses = np.zeros((len(wes), 6))\n",
    "i=0\n",
    "\n",
    "for we in wes:\n",
    "    mses = train_test_graph_pipeline(we, y_train)# fix this function\n",
    "    mses[i] = mse\n",
    "    i+=1\n",
    "\n",
    "print(mses)\n",
    "mses = np.around(mses, decimals=2)\n",
    "np.savetxt('word_embedding_results.csv', mses, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Different feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all possible combinations\n",
    "\n",
    "(a_ab, b_ab) = (a1, b1)\n",
    "(a_gr, b_gr) = (a2, b2)\n",
    "(a_wa, b_wa) = (a3, b3)\n",
    "(a_ab_gr, b_ab_gr) = (np.concatenate((a_ab, a_gr), axis=1), np.concatenate((b_ab, b_gr), axis=1))\n",
    "(a_ab_wa, b_ab_wa) = (np.concatenate((a_ab, a_wa), axis=1), np.concatenate((b_ab, b_wa), axis=1))\n",
    "(a_gr_wa, b_ab_wa) = (np.concatenate((a_gr, a_wa), axis=1), np.concatenate((b_gr, b_wa), axis=1))\n",
    "(a_all, b_all) = (np.concatenate((a_ab, a_gr_wa),axis =1), np.concatenate((a_ab, a_gr_wa), axis=1))\n",
    "\n",
    "comb = [(a_ab, b_ab), (a_gr, b_gr), (a_wa, b_wa), (a_ab_gr, b_ab_gr), (a_ab_wa, b_ab_wa), (a_gr_wa, b_ab_wa), (a_all, b_all)]\n",
    "\n",
    "mses  = np.zeros((7, 6))\n",
    "i=0\n",
    "\n",
    "for (X_train, X_test) in comb:\n",
    "    X_train_1, y_train_1, X_validation, y_validation = shuffle_split(X_train, y_train)\n",
    "    mse = train_test_graph_pipeline('No', y_train, X_train_1 = X_train_1, y_train_1 = y_train_1, X_validation = X_validation, y_validation = y_validation)\n",
    "    mses[i] = mse\n",
    "    i+=1\n",
    "    print(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.72300000e+01 1.15930000e+02 1.01020000e+02 1.00960000e+02\n",
      "  1.43500000e+02 9.33600000e+01]\n",
      " [1.09970000e+02 1.31090000e+02 1.16180000e+02 1.31020000e+02\n",
      "  2.86816982e+22 1.08430000e+02]\n",
      " [9.04200000e+01 1.57710000e+02 1.14740000e+02 1.14730000e+02\n",
      "  1.17140000e+02 1.13940000e+02]\n",
      " [5.83700000e+01 1.07150000e+02 9.61700000e+01 9.61600000e+01\n",
      "  1.75570283e+26 7.42400000e+01]\n",
      " [5.71400000e+01 1.18600000e+02 8.20600000e+01 8.20300000e+01\n",
      "  9.71700000e+01 8.44900000e+01]\n",
      " [8.34000000e+01 1.23910000e+02 1.00950000e+02 1.00940000e+02\n",
      "  2.87923367e+22 9.27100000e+01]\n",
      " [5.67100000e+01 1.07630000e+02 8.09000000e+01 8.08900000e+01\n",
      "  2.31323571e+26 7.34300000e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(mses)\n",
    "table_2 = np.around(mses, decimals=2)\n",
    "np.savetxt('combinations_results.csv', table_2, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_pred_1, y_validation):\n",
    "    mse = (np.square(y_pred_1 - y_validation)).mean(axis=0)\n",
    "    print('MSE =',  mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=(200), activation='relu', solver='adam', verbose = 1, max_iter=70, n_iter_no_change=10, early_stopping = True)#use pytorch implementation instead (way faster)\n",
    "\n",
    "#on split test/validation\n",
    "reg.fit(X_train_1, y_train_1)\n",
    "\n",
    "#whoile training set\n",
    "#reg.fit(X_train, y_train)\n",
    "\n",
    "plt.plot(reg.loss_curve_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 layers -> 56.98129538854695\n",
    "#150 layers - >  56.93900433307325\n",
    "#100 layers -> 56.1059605875759\n",
    "#75 layers -> MSE = 57.55111207305141\n",
    "#125 layers -> MSE = 56.478676360493374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data predicted\n",
      "MSE = 57.346453434697146\n"
     ]
    }
   ],
   "source": [
    "#compute mse for validation set (only do it when training with training/validation)\n",
    "y_pred_1 = reg.predict(X_validation)\n",
    "print('data predicted')\n",
    "compute_mse(y_pred_1, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 out of   8 | elapsed: 15.8min remaining: 47.5min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(200), activation='relu', solver='adam', verbose = 1, max_iter=100, n_iter_no_change=15, early_stopping = True)\n",
    "reg = BaggingRegressor(mlp, n_estimators= 8, n_jobs=-1, verbose = 1)\n",
    "\n",
    "#on split test/validation\n",
    "#reg.fit(X_train_1, y_train_1)\n",
    "\n",
    "#on whole training set\n",
    "reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute mse for validation set (only do it when training with training/validation)\n",
    "y_pred_1 = reg.predict(X_validation)\n",
    "print('data predicted')\n",
    "compute_mse(y_pred_1, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done   2 out of  16 | elapsed:    8.0s remaining:   56.4s\n",
      "[Parallel(n_jobs=16)]: Done  16 out of  16 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data written\n"
     ]
    }
   ],
   "source": [
    "# write the predictions to file\n",
    "y_pred = reg.predict(X_test)\n",
    "y_pred[y_pred < 0] = 0 #post-processing\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
    "print('data written')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345a0f5943a8075ed7616e2c3d9d78f5a99e0a39cacecea04843f429e3a92ccf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test_karate': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
