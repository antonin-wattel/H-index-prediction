{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alexandre Hirsch, Antonin Wattel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import json\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import datapath\n",
    "#import nltk\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.9s\n",
    "\n",
    "# read training data\n",
    "df_train = pd.read_csv('train.csv', dtype={'author': np.int64, 'hindex': np.float32})\n",
    "n_train = df_train.shape[0]\n",
    "\n",
    "# read test data\n",
    "df_test = pd.read_csv('test.csv', dtype={'author': np.int64})\n",
    "n_test = df_test.shape[0]\n",
    "\n",
    "# # load the graph    \n",
    "# G = nx.read_edgelist('coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "# n_nodes = G.number_of_nodes()\n",
    "# n_edges = G.number_of_edges() \n",
    "# print('Number of nodes:', n_nodes)\n",
    "# print('Number of edges:', n_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text data (abstracts.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1m40s\n",
    "\n",
    "#try with different pre-trained word-embedding models\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "#wv = api.load('glove-wiki-gigaword-300')\n",
    "print(\"Word2Vec model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our own word2vec model on the abstracts \n",
    "#sentences2 = []\n",
    "#sentences2 = [[word for word in nltk.word_tokenize(s) if word.isalnum()] for s in sentences]\n",
    "#model = Word2Vec(sentences2, sg = 1)\n",
    "#print(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2m 40s\n",
    "\n",
    "def store_abstracts():\n",
    "    paper_IDs = dict()\n",
    "    #with open('abstracts.txt') as f:\n",
    "    with open('abstracts.txt', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            paper_ID, abstract = l.split(\"----\",1)\n",
    "            #sent = [None]*d[\"IndexLength\"]\n",
    "            #for word, v in d[\"InvertedIndex\"].items():\n",
    "                #for i in v:\n",
    "                    #sent[i] = word\n",
    "            #sentences += nltk.sent_tokenize(' '.join(list(filter(None, sent))))\n",
    "            paper_IDs[int(paper_ID)] = json.loads(abstract)[\"InvertedIndex\"].keys() - STOPWORDS\n",
    "    return paper_IDs #, sentences\n",
    "\n",
    "paper_IDs = store_abstracts()\n",
    "print('abstracts stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 s\n",
    "\n",
    "def store_authors():\n",
    "    author_IDs = dict()\n",
    "    with open('author_papers.txt') as f:\n",
    "        for l in f:\n",
    "            author_ID, papers = l.split(':')\n",
    "            author_IDs[int(author_ID)] = map(int,papers.split('-'))\n",
    "    return author_IDs\n",
    "    \n",
    "author_IDs = store_authors()\n",
    "print('authors stored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_value(paper_ID):\n",
    "    vec = np.zeros(wv.vector_size)\n",
    "    try:\n",
    "        words_used = set()\n",
    "        for token in paper_IDs[paper_ID]:\n",
    "            words = re.sub(r'[-/]', ' ', re.sub(r'[.…,:?!;\\'‘’\"“”()*–]|[0-9]+-|[0-9]|\\'s', '', token))\n",
    "            for w in words.split():\n",
    "                if w not in STOPWORDS and w not in words_used:\n",
    "                    words_used.add(w)\n",
    "                    try:\n",
    "                        vec += wv[w]\n",
    "                    except:\n",
    "                        continue\n",
    "    except:\n",
    "        pass\n",
    "    return vec\n",
    "\n",
    "def get_author_value(author_ID):\n",
    "    vec = np.zeros(wv.vector_size)\n",
    "    for paper_ID in author_IDs[author_ID]:\n",
    "        vec += get_paper_value(paper_ID)\n",
    "    return vec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "X_train = np.zeros((n_train, wv.vector_size))\n",
    "#y_train = np.zeros(n_train)\n",
    "\n",
    "for i,row in df_train.iterrows():\n",
    "    author = row['author']\n",
    "    X_train[i,:] = get_author_value(author)\n",
    "    #y_train[i] = row['hindex']\n",
    "\n",
    "print('training data loaded')\n",
    "\n",
    "#this is useless for the moment----------\n",
    "# X_train_1 = X_train[:len(df_train)//5]\n",
    "# y_train_1 = y_train[:len(df_train)//5]\n",
    "\n",
    "# X_validation = X_train[len(df_train)//5:]\n",
    "# y_validation = y_train[len(df_train)//5:]\n",
    "#-------------------------------------------\n",
    "\n",
    "\n",
    "X_test = np.zeros((n_test, wv.vector_size))\n",
    "for i,row in df_test.iterrows():\n",
    "    author = row['author']\n",
    "    X_test[i,:] = get_author_value(author)\n",
    "print('testing data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape)\n",
    "# print(X_train_1.shape)\n",
    "# print(X_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save feature vectors from the abstracts\n",
    "\n",
    "\n",
    "np.save('X_train_abstract.npy', X_train)\n",
    "np.save('X_test_abstract.npy', X_test)\n",
    "\n",
    "del X_train\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10s\n",
    "\n",
    "# load the graph    \n",
    "G = nx.read_edgelist('coauthorship.edgelist', delimiter=' ', nodetype=int)\n",
    "n_nodes = G.number_of_nodes()\n",
    "n_edges = G.number_of_edges() \n",
    "print('Number of nodes:', n_nodes)\n",
    "print('Number of edges:', n_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features (no learning) -> centrality based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#careful: this takes time to run !!!! ()\n",
    "\n",
    "#https://arxiv.org/ftp/arxiv/papers/1911/1911.08795.pdf \n",
    "#https://www.geeksforgeeks.org/network-centrality-measures-in-a-graph-using-networkx-python/\n",
    "\n",
    "\n",
    "#let's just hope there is enough space in memory for all this\n",
    "\n",
    "core_number = nx.core_number(G)\n",
    "print('core_number')\n",
    "page_rank = nx.pagerank(G, alpha=0.9) #not so sure about the alpha\n",
    "print('pagerank')\n",
    "nb_triangles = nx.triangles(G)\n",
    "print('triangles')\n",
    "deg_centrality = nx.degree_centrality(G) \n",
    "print('deg_centrality')\n",
    "# close_centrality = nx.closeness_centrality(G)#veerrry long >40 m\n",
    "# print('closeness_centrality')\n",
    "# bet_centrality = nx.betweenness_centrality(G, normalized = True, endpoints = False)\n",
    "# print('bet_centrality')\n",
    "# eig_centrality = nx.eigenvector_centrality(G)\n",
    "# print('eigenvector_centrality')\n",
    "# katz_centrality = nx.katz_centrality(G)\n",
    "# print('katz_centrality')\n",
    "# current_flow_closeness_centrality = nx.current_flow_closeness_centrality(G)\n",
    "# print('current_flow')\n",
    "# current_flow_betweenness_centrality = nx.current_flow_betweenness_centrality(G)\n",
    "# print('current_flow_betweenness_centrality')\n",
    "# load_centrality = nx.load_centrality(G)\n",
    "# print('load_centrality')\n",
    "# harmonic_centrality = nx.harmonic_centrality(G)\n",
    "# print('harmonic')\n",
    "# percolation_centrality = nx.percolation_centrality(G)\n",
    "# print('percolation')\n",
    "# second_order_centrality = nx.second_order_centrality(G)\n",
    "# print('second oreder')\n",
    "# trophic_levels = nx.trophic_levels(G)\n",
    "# print('trophic levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((n_train, 17))\n",
    "\n",
    "for i,row in df_train.iterrows():\n",
    "    author = row['author']\n",
    "    X_train[i, 0] = G.degree(author)\n",
    "    X_train[i, 1] = core_number[author]\n",
    "    X_train[i, 2] = page_rank[author]\n",
    "    X_train[i, 3] = nb_triangles[author]\n",
    "    X_train[i, 4] = deg_centrality[author]\n",
    "    # X_train[i, 5] = close_centrality[author]\n",
    "    # X_train[i, 6] = bet_centrality[author]\n",
    "    # X_train[i, 7] = eig_centrality[author]\n",
    "    # X_train[i, 8] = katz_centrality[author]\n",
    "    # X_train[i, 9] = current_flow_closeness_centrality[author]\n",
    "    # X_train[i, 10] = current_flow_betweenness_centrality[author]\n",
    "    # X_train[i, 11] = load_centrality[author]\n",
    "    # X_train[i, 12] = harmonic_centrality[author]\n",
    "    # X_train[i, 13] = percolation_centrality[author]\n",
    "    # X_train[i, 14] = second_order_centrality[author]\n",
    "    # X_train[i, 15] = trophic_levels[author]\n",
    "\n",
    "    #TO DO: largest clique number (he feature of a node is defined as the largest k of a k-clique that contains that node.)\n",
    "print('loaded training features')\n",
    "\n",
    "#-----------------------------\n",
    "\n",
    "X_test = np.zeros((n_test, 17))\n",
    "\n",
    "for i,row in df_test.iterrows():\n",
    "    author = row['author']\n",
    "    X_test[i, 0] = G.degree(author)\n",
    "    X_test[i, 1] = core_number[author]\n",
    "    X_test[i, 2] = page_rank[author]\n",
    "    X_test[i, 3] = nb_triangles[author]\n",
    "    X_test[i, 4] = deg_centrality[author]\n",
    "    # X_test[i, 5] = close_centrality[author]\n",
    "    # X_test[i, 6] = bet_centrality[author]\n",
    "    # X_test[i, 7] = eig_centrality[author]\n",
    "    # X_test[i, 8] = katz_centrality[author]\n",
    "    # X_test[i, 9] = current_flow_closeness_centrality[author]\n",
    "    # X_test[i, 10] = current_flow_betweenness_centrality[author]\n",
    "    # X_test[i, 11] = load_centrality[author]\n",
    "    # X_test[i, 12] = harmonic_centrality[author]\n",
    "    # X_test[i, 13] = percolation_centrality[author]\n",
    "    # X_test[i, 14] = second_order_centrality[author]\n",
    "    # X_test[i, 15] = trophic_levels[author]\n",
    "\n",
    "print('loaded testing features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_graph.npy', X_train)\n",
    "np.save('X_test_graph.npy', X_test)\n",
    "# np.save('X_train_1_graph.npy', X_train)\n",
    "# np.save('X_validation_graph.npy', X_train)\n",
    "\n",
    "del X_train\n",
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use different graph methods to extract node embeddings from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_ = G.copy()\n",
    "#original_indices = [node for node in G.nodes()]\n",
    "#print(\"original = \", original_indices[0:10])\n",
    "#G = nx.convert_node_labels_to_integers(G, first_label=0, ordering='default')\n",
    "\n",
    "#to do: make sure we get bak to the same\n",
    "\n",
    "\n",
    "#problem !!! we need to use an ordered dict here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert IDs to numeric indices in order to use karate library\n",
    "\n",
    "mapping_to_indices = { node : i for node, i in zip([node for node in G.nodes()], [i for i in range(n_nodes)]) }\n",
    "inverse_mapping = dict(zip(mapping_to_indices.values(),mapping_to_indices.keys()))\n",
    "G = nx.relabel_nodes(G, mapping_to_indices)\n",
    "node_indices = [node for node in G.nodes()]\n",
    "print(\"node = \", node_indices[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do: make a for loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a few hours to run...\n",
    "from karateclub.node_embedding.neighbourhood import Node2Vec \n",
    "\n",
    "model1 = Node2Vec()\n",
    "model1.fit(G)\n",
    "print('fitted Node2Vec')\n",
    "\n",
    "embedding2 = model1.get_embedding()\n",
    "np.save('Node2Vec_embedding.npy', embedding2 )\n",
    "\n",
    "del embedding2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#about 20 m\n",
    "from karateclub.node_embedding.neighbourhood import BoostNE\n",
    "\n",
    "model2 = BoostNE()\n",
    "model2.fit(G)\n",
    "print('fitted Node2Vec')\n",
    "\n",
    "embedding2 = model2.get_embedding()\n",
    "np.save('BoostNE_embedding.npy', embedding2 )\n",
    "\n",
    "del embedding2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import NetMF\n",
    "\n",
    "model3 = NetMF()\n",
    "model3.fit(G)\n",
    "print('fitted Node2Vec')\n",
    "\n",
    "embedding2 = model3.get_embedding()\n",
    "np.save('NetMF_embedding.npy', embedding2  )\n",
    "\n",
    "del embedding2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import DeepWalk\n",
    "\n",
    "model = BoostNE()\n",
    "model.fit(G)\n",
    "print('fitted Node2Vec')\n",
    "\n",
    "embedding2 = model.get_embedding()\n",
    "np.save('DeepWalk_embedding.npy', embedding2  )\n",
    "\n",
    "del embedding2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 s\n",
    "\n",
    "from karateclub.node_embedding.neighbourhood import RandNE\n",
    "\n",
    "model = RandNE()\n",
    "model.fit(G)\n",
    "print('fitted Node2Vec')\n",
    "\n",
    "embedding2 = model.get_embedding()\n",
    "np.save('RandNE_embedding.npy', embedding2  )\n",
    "\n",
    "del embedding2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub.node_embedding.neighbourhood import GraRep\n",
    "\n",
    "model = GraRep()\n",
    "model.fit(G)\n",
    "print('fitted Node2Vec')\n",
    "\n",
    "embedding2 = model.get_embedding()\n",
    "np.save('GraRep.npy', embedding2  )\n",
    "\n",
    "del embedding2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_indices = np.load('original_indices.npy')\n",
    "#to d0: make this parralel\n",
    "\n",
    "# def load_data(embedding):\n",
    "#     X_train = np.zeros((n_train, embedding.shape[1]))\n",
    "#     #y_train = np.zeros(n_train)     \n",
    "    \n",
    "#     for i,row in df_train.iterrows():\n",
    "#         author = row['author']\n",
    "\n",
    "#         j = int(np.where(original_indices == author)[0]) #I guess this is really not efficient\n",
    "#         X_train[i,:] = embedding[j]#not index i #we need to access node with author_id \n",
    "\n",
    "#         #y_train[i] = row['hindex'] #We loaded this before\n",
    "\n",
    "#     print('training data loaded')\n",
    "\n",
    "#     X_train_1 = X_train[:len(df_train)//5]\n",
    "#     X_validation = X_train[len(df_train)//5:]\n",
    "\n",
    "\n",
    "#     X_test = np.zeros((n_test, wv.vector_size))\n",
    "#     for i,row in df_test.iterrows():\n",
    "#         author = row['author']\n",
    "#         j = np.where(np.array(original_indices) == author)[0]\n",
    "#         X_test[i,:] = embedding[j]# not the right order ?\n",
    "#     print('testing data loaded')\n",
    "\n",
    "#     return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #choose an embedding\n",
    "# embedding = np.load('Deepwalk_embedding_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = load_data(embedding)\n",
    "# np.save('X_train_'+str(embedding)+'.npy', X_train)\n",
    "# np.save('X_test_'+str(embedding)+'.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a relabelling test\n",
    "G_ = nx.relabel_nodes(G, inverse_mapping)\n",
    "node_indices_ = [node for node in G.nodes()]\n",
    "print(\"node = \", node_indices_[0:10])\n",
    "#we have ids again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 s\n",
    "\n",
    "#make sure this is right\n",
    "def load_and_save_from_embedding(embedding_string):\n",
    "\n",
    "    path = embedding_string+ '_embedding.npy'\n",
    "    if not os.path.exists(path):\n",
    "        print(path+ 'does not exist')\n",
    "        return 0\n",
    "        \n",
    "    embedding = np.load(path) \n",
    "    #to do: try_ except\n",
    "\n",
    "    X_train = np.zeros((n_train, len(embedding[0])))\n",
    "    y_train = np.zeros(n_train)\n",
    "    #print(X_train.shape)\n",
    "\n",
    "    for i,row in df_train.iterrows():\n",
    "        author = row['author']\n",
    "        #j = int(np.where(original_indices == author)[0]) #I guess this is really not efficient\n",
    "        j = mapping_to_indices[author]#not so sure\n",
    "        X_train[i,:] = embedding[j]\n",
    "        #y_train[i] = row['hindex']\n",
    "\n",
    "    #print('training data loaded')\n",
    "\n",
    "    X_test = np.zeros((n_test, len(embedding[0])))\n",
    "    for i,row in df_test.iterrows():\n",
    "        author = row['author']\n",
    "        #j = np.where(np.array(original_indices) == author)[0]\n",
    "        j = mapping_to_indices[author]\n",
    "        X_test[i,:] = embedding[j]\n",
    "\n",
    "    #print('testing data loaded')\n",
    "\n",
    "    np.save('X_train_'+embedding_string+'.npy', X_train)\n",
    "    np.save('X_test_'+embedding_string+'.npy', X_test)\n",
    "\n",
    "    del X_train\n",
    "    del X_test\n",
    "    gc.collect()    \n",
    "\n",
    "    print(embedding_string+' done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list = ['Node2Vec', 'GraRep', 'BoostNE', 'NetMF', 'RandNE', 'Deepwalk']\n",
    "\n",
    "for embedding in embedding_list:\n",
    "    load_and_save_from_embedding(embedding)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Now that we have features, we can assemble them and perform regression to get our prediction model.<br/>\n",
    "here, we load pre-computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros(n_train)\n",
    "for i,row in df_train.iterrows():\n",
    "    y_train[i] = row['hindex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: CHANGE THIS -> automatize this in order to stack any features easily\n",
    "\n",
    "a1 = np.load('X_train_abstract.npy')\n",
    "a2 = np.load('X_train_graph.npy')\n",
    "a3 = np.load('X_train_Deepwalk.npy')\n",
    "\n",
    "b1 = np.load('X_test_abstract.npy')\n",
    "b2 = np.load('X_test_graph.npy')\n",
    "b3 = np.load('X_test_Deepwalk.npy')\n",
    "\n",
    "\n",
    "#might meed normalization... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack features from the abstract and the graph\n",
    "\n",
    "X_train = np.concatenate((a1, a2, a3), axis=1)\n",
    "print(X_train.shape)\n",
    "X_test = np.concatenate((b1, b2, b3), axis=1)\n",
    "print(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training set into training_1 and validation\n",
    "\n",
    "#shuffle\n",
    "s = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(s)\n",
    "X_train  = X_train[s]\n",
    "y_train = y_train[s]\n",
    "\n",
    "#split\n",
    "X_train_1 = X_train[len(df_train)//5:]\n",
    "y_train_1 = y_train[len(df_train)//5:]\n",
    "X_validation= X_train[:len(df_train)//5]\n",
    "y_validation = y_train[:len(df_train)//5]\n",
    "\n",
    "#is this of any use ?\n",
    "# #X_test = np.zeros((n_test, wv.vector_size))\n",
    "# X_test = np.zeros((n_test, 1))\n",
    "# for i,row in df_test.iterrows():\n",
    "#     author = row['author']\n",
    "#     X_test[i,:] = get_author_value(author)\n",
    "# print('testing data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----- train:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('----- train_1')\n",
    "print(X_train_1.shape)\n",
    "print(y_train_1.shape)\n",
    "print('----- validation')\n",
    "print(X_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print('----- test')\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on X_train_1 and y_train_1\n",
    "(we do this to evaluate the MLE with a validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import sklearn\n",
    "\n",
    "#reg = Lasso(alpha=0.001, max_iter=1000)\n",
    "#several hidden layers ?\n",
    "reg = MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, verbose = 1, max_iter=20, tol = 1*pow(10, -6), n_iter_no_change=20)#use pytorch implementation instead (way faster)\n",
    "\n",
    "#reg = sklearn.linear_model.PassiveAggressiveRegressor(verbose=1)#nul\n",
    "# reg = sklearn.linear_model.HuberRegressor(max_iter=200)#supposed to be robust to outliers (not good)\n",
    "#reg = sklearn.linear_model.ARDRegression(verbose=1)#MSE=86 in 10 iterations\n",
    "#reg = sklearn.linear_model.BayesianRidge(verbose=1) #MSE=86 in 8 iterations\n",
    "#reg = sklearn.svm.LinearSVR(verbose=1) #Linear Support Vector Regression. (takes 3m with default params) -> not good at all\n",
    "\n",
    "#these ones dont seem to work\n",
    "#reg = sklearn.gaussian_process.GaussianProcessRegressor(verbose=1)\n",
    "#reg = sklearn.isotonic.IsotonicRegression()\n",
    "#reg = sklearn.kernel_ridge.KernelRidge()\n",
    "\n",
    "\n",
    "#ensemble methods (meta learning.) is it that useful ???\n",
    "\n",
    "#careful: this will take some time.\n",
    "#to do: use this on final model that works best\n",
    "\n",
    "base = MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, verbose = 1, max_iter=20, tol = 1*pow(10, -6), n_iter_no_change=20)#use pytorch implementation instead (way faster)\n",
    "base = Lasso(alpha=0.001, max_iter=5)\n",
    "reg = AdaBoostRegressor(base_estimator = reg, random_state=0, n_estimators=5)\n",
    "\t\n",
    "\n",
    "print(\"regressor loaded\")\n",
    "reg.fit(X_train_1, y_train_1)\n",
    "print('data fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do: see sklearn.linear_model\n",
    "#-> provides many nice regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?\n",
    "\n",
    "# #Gaussian process regression (GPR).\n",
    "# gaussian_process.GaussianProcessRegressor([…])\n",
    "# #Isotonic regression model.\n",
    "# isotonic.IsotonicRegression(*[, y_min, …])\n",
    "# #Kernel ridge regression.\n",
    "# kernel_ridge.KernelRidge([alpha, kernel, …])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see ensemble methods for regression in scikit learn\n",
    "#->understand how this works\n",
    "\n",
    "# sklearn.ensemble.AdaBoostRegressor([base_estimator, …])\n",
    "# ensemble.BaggingRegressor([base_estimator, …])\n",
    "# ensemble.ExtraTreesRegressor([n_estimators, …])\n",
    "# ensemble.GradientBoostingRegressor(*[, …])\n",
    "# ensemble.RandomForestRegressor([…])\n",
    "# ensemble.StackingRegressor(estimators[, …])\n",
    "# ensemble.VotingRegressor(estimators, *[, …])\n",
    "# ensemble.HistGradientBoostingRegressor([…])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_pred_1, y_validation):\n",
    "    mse = (np.square(y_pred_1 - y_validation)).mean(axis=0)\n",
    "    print(y_pred_1[10:20])\n",
    "    print(y_validation[10:20])\n",
    "    print('MSE =',  mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = reg.predict(X_validation)\n",
    "print('data predicted')\n",
    "\n",
    "compute_mse(y_pred_1, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to to: cast the final values as integers then floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on whole X_train\n",
    "(used for submission for better accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "reg_final = Lasso(alpha=0.001)\n",
    "#might want to change batch size\n",
    "#might want to have several hidden layers\n",
    "#reg_final = MLPRegressor(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001, max_iter=400, verbose=1, tol = 1*pow(10, -1))#use pytorch implementation instead (way faster)\n",
    "\n",
    "print(\"regressor loaded\")\n",
    "reg_final.fit(X_train, y_train)\n",
    "print('data fitted')\n",
    "y_pred = reg_final.predict(X_test)\n",
    "print('data predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the predictions to file\n",
    "df_test['hindex'] = pd.Series(np.round_(y_pred, decimals=3))\n",
    "\n",
    "df_test.loc[:,[\"author\",\"hindex\"]].to_csv('submission.csv', index=False)\n",
    "print('data written')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE AN MLP FROM PYTORCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on X_train_1 and y_train_1 using pytorch MLP\n",
    "(we do this to evaluate the MLE without needing to submit on kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  '''\n",
    "  Prepare the Boston dataset for regression\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X, y, scale_data=True):\n",
    "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "      # Apply scaling if necessary\n",
    "      if scale_data:\n",
    "          X = StandardScaler().fit_transform(X)\n",
    "      self.X = torch.from_numpy(X)\n",
    "      self.y = torch.from_numpy(y)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      \n",
    "      nn.Linear(X_train_1.shape[1], 150),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(150, 1),\n",
    "      #nn.ReLU(),\n",
    "     #nn.Dropout(0.5),\n",
    "      #nn.Linear(60, 1),\n",
    "      \n",
    "      # nn.ReLU(),\n",
    "      # nn.Linear(32, 1),\n",
    "      # nn.Dropout(0.8)\n",
    "      #nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#might want to change the baseline\n",
    "\n",
    "# Prepare dataset\n",
    "# dataset = Dataset(X_train, y_train)\n",
    "# trainloader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True, num_workers=0)\n",
    "\n",
    "#training (with validation)\n",
    "dataset_train_1 = Dataset(X_train_1, y_train_1)\n",
    "trainloader = torch.utils.data.DataLoader(dataset_train_1, batch_size=300, shuffle=True, num_workers=0)\n",
    "\n",
    "#validation\n",
    "dataset_valid = Dataset(X_validation, y_validation)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=300, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize the MLP\n",
    "mlp = MLP().to(device)\n",
    "# Define the loss function and optimizer\n",
    "#loss_function = nn.L1Loss()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3, weight_decay=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "\n",
    "def train_model(model, optizmizer, loss_function):\n",
    "    iter = 0\n",
    "    num_epochs = 100\n",
    "    history_train_acc, history_val_acc, history_train_loss, history_val_loss = [], [], [], []\n",
    "    #best_accuracy = 0\n",
    "\n",
    "    # Run the training loop\n",
    "    for epoch in range(num_epochs): \n",
    "      \n",
    "      # Print epoch\n",
    "      print(f'Starting epoch {epoch+1}')\n",
    "      \n",
    "      # Set current loss value\n",
    "      current_loss = 0.0\n",
    "      \n",
    "      # Iterate over the DataLoader for training data\n",
    "      for i, data in enumerate(trainloader):\n",
    "        best_val_loss = 10**10000\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Perform forward pass\n",
    "        #outputs = mlp(inputs)\n",
    "        #print('inputs: ', inputs.shape)\n",
    "       # print('model.layers', model.layers)\n",
    "        outputs = model(inputs)\n",
    "        #print('passed forward pass')\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 100 == 0:\n",
    "              # Get training statistics\n",
    "              train_loss = loss.data.item()\n",
    "\n",
    "              # Testing mode\n",
    "              model.eval()\n",
    "              # Calculate Accuracy         \n",
    "              correct = 0\n",
    "              total = 0\n",
    "              # Iterate through test dataset\n",
    "              for inputs, targets in valid_loader:\n",
    "                  # Load samples\n",
    "                  #samples = samples.view(-1, max_len).to(device)\n",
    "                  #labels = labels.view(-1).to(device)\n",
    "\n",
    "                  #inputs, targets = data\n",
    "                  inputs, targets = inputs.float().to(device), targets.float().to(device)\n",
    "                  targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "                  # Forward pass only to get logits/output\n",
    "                  outputs = model(inputs)\n",
    "\n",
    "                  # Val loss\n",
    "                  #val_loss = criterion(outputs.view(-1, 1), la.bels.view(-1, 1))\n",
    "                  val_loss = loss_function(outputs, targets)\n",
    "        # Print Loss\n",
    "              print('Iter: {} | Train Loss: {} | Val Loss: {} '.format(iter, train_loss, val_loss.item()))\n",
    "              # Append to history\n",
    "              history_val_loss.append(val_loss.data.item())\n",
    "              #history_val_acc.append(round(accuracy, 2))\n",
    "              history_train_loss.append(train_loss)\n",
    "\n",
    "              #to do: save the best model:\n",
    "              #we take the model with lowest \n",
    "              # Save model when accuracy beats best accuracy (TO DO !!!!)\n",
    "              if val_loss.data.item() < best_val_loss:\n",
    "                   best_val_loss = val_loss.data.item()\n",
    "              #     # We can load this best model on the validation set later\n",
    "                   torch.save(model.state_dict(), 'best_model.pth')\n",
    "    return (history_train_acc, history_val_acc, history_train_loss, history_val_loss)\n",
    "\n",
    "\n",
    "    #     # Print statistics\n",
    "    #     current_loss += loss.item()\n",
    "    #     if i % 1000 == 0:\n",
    "    #         print('Loss after mini-batch %5d: %.3f' %\n",
    "    #               (i + 1, current_loss / 500))\n",
    "    #         current_loss = 0.0\n",
    "\n",
    "    # # Process is complete.\n",
    "    # print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to update\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_losses(history_train_loss, history_val_loss):\n",
    "    # Set plotting style\n",
    "    #plt.style.use(('dark_background', 'bmh'))\n",
    "    plt.style.use('bmh')\n",
    "    plt.rc('axes', facecolor='none')\n",
    "    plt.rc('figure', figsize=(16, 4))\n",
    "\n",
    "    # Plotting loss graph\n",
    "    plt.plot(history_train_loss, label='Train')\n",
    "    plt.plot(history_val_loss, label='Validation')\n",
    "    plt.title('Loss Graph')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_acc, val_acc, train_loss, val_loss) = train_model(mlp, optimizer, loss_function)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = mlp\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_train = mlp(torch.from_numpy(X_validation).float()).detach().numpy().T[0]\n",
    "\n",
    "best_model.cpu()\n",
    "mlp.cpu()\n",
    "y_pred_1 = best_model(torch.from_numpy(X_validation).float()).detach().numpy().T[0]\n",
    "#print(y_pred_1)\n",
    "#print(y_validation)\n",
    "compute_mse(y_pred_1, y_validation)\n",
    "#wtf ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO:\n",
    "\n",
    "#- split X_train, y_train into train and test, so we can evaluate the MSE\n",
    "\n",
    "# idea of features to add: (no learning)\n",
    "# - number of papers\n",
    "# - average of  number of papers of an author's co-author (will need the graph for this)\n",
    "\n",
    "# add graph features: (no learning)\n",
    "# see -> https://arxiv.org/ftp/arxiv/papers/1911/1911.08795.pdf\n",
    "\n",
    "#run and save node embeddings with different methods (Node2Vec, Deepwalk..) using karateclub libary \n",
    "#try to play with the paramters\n",
    "#record execution times\n",
    "\n",
    "#- look for new features/methods from texts. \n",
    "#- different ways of combining word embeddings ?\n",
    "#- train word embeddings on our dataset ?\n",
    "\n",
    "# - do a pipeline and test every possible combination of features\n",
    "# - (as done there https://github.com/vanessachahwan/ALTEGRAD-Challenge/blob/main/Report-BMV.pdf section 6)\n",
    "\n",
    "# - compare different regression models\n",
    "# - fine tune parameters for regression model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345a0f5943a8075ed7616e2c3d9d78f5a99e0a39cacecea04843f429e3a92ccf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test_karate': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
